# LLMサービス実装概要

このドキュメントは、2025年6月16日時点の`doc_ai_helper_backend`プロジェクトにおけるLLM（大規模言語モデル）サービス層の実装について概要をまとめたものです。

## 概要

LLMサービス層は、クリーンな抽象化レイヤーを通じて様々なLLMプロバイダー（OpenAI、Anthropicなど）と対話するための統一インターフェースを提供します。実装は関心事の明確な分離を持つモジュラー設計に従っています：

1. **抽象化**：プロバイダー選択のための基本インターフェースとファクトリーパターン
2. **キャッシュ**：API呼び出しを削減するためのLLMレスポンスキャッシュ
3. **コンテキスト管理**：標準化されたコンテキスト処理のためのModel Context Protocol（MCP）アダプター
4. **テンプレート**：プロンプトテンプレート管理システム
5. **API統合**：LLM機能を公開するFastAPIエンドポイント

## コンポーネント

### 1. ベースLLMサービス（`base.py`）

`LLMServiceBase`抽象クラスは、すべてのLLMサービス実装に共通するインターフェースを以下の主要メソッドで定義しています：
- `query()`：LLMプロバイダーにプロンプトを送信
- `get_capabilities()`：プロバイダーの機能を取得
- `format_prompt()`：変数を使用してプロンプトテンプレートをフォーマット
- `estimate_tokens()`：価格/制限のためのトークン数を推定

### 2. LLMサービスファクトリー（`factory.py`）

`LLMServiceFactory`は適切なLLMサービスインスタンスを作成するためのファクトリーパターンを実装しています：
- プロバイダーの動的登録をサポート
- プロバイダー設定を管理
- サービス検出を提供

### 3. モックLLMサービス（`mock_service.py`）

`MockLLMService`はテスト用の実装を提供します：
- テスト用の予測可能なレスポンスを返す
- API呼び出しなしでプロバイダーの動作をシミュレート
- エッジケースをテストするための設定可能な遅延とエラーを含む

### 4. キャッシュサービス（`cache_service.py`）

`LLMCacheService`はLLMレスポンスのためのインメモリキャッシュを実装しています：
- プロンプトとオプションから決定論的なキャッシュキーを生成
- キャッシュアイテムのTTL（Time-To-Live）を実装
- キャッシュと期限切れアイテムをクリアするためのメソッドを提供

### 5. MCPアダプター（`mcp_adapter.py`）

`MCPAdapter`はModel Context Protocolに従ってコンテキストのフォーマットを処理します：
- ドキュメントを標準化されたコンテキスト形式に変換
- トークン制限内に収まるようにコンテキストを最適化
- 関連性に基づいてコンテンツの優先順位付け

### 6. テンプレートマネージャー（`template_manager.py`）

`PromptTemplateManager`はプロンプトテンプレートを管理するシステムを提供します：
- JSONファイルからテンプレートを読み込み
- 変数置換によるテンプレートのフォーマット
- 必須変数の検証

### 7. APIエンドポイント（`llm.py`）

APIエンドポイントはRESTfulインターフェースを通じてLLM機能を公開します：
- `/query`：コンテキスト付きでLLMにクエリを送信
- `/capabilities`：プロバイダーの機能を取得
- `/templates`：利用可能なテンプレートをリスト
- `/format-prompt`：プロンプトテンプレートをフォーマット

## データモデル

LLMサービスの主要なデータモデルには以下が含まれます：
- `LLMQueryRequest`：LLMクエリのリクエストモデル
- `LLMResponse`：標準化された構造を持つレスポンスモデル
- `LLMUsage`：課金/モニタリングのためのトークン使用情報
- `PromptTemplate`：変数を持つテンプレート定義

## テスト状況

### ユニットテスト（成功）
- `test_llm_services.py`：ファクトリーとモックサービスのテスト
- `test_template_manager.py`：テンプレートの読み込みとフォーマットのテスト
- `test_mcp_adapter.py`：コンテキスト変換と最適化のテスト
- `test_cache_service.py`：キャッシュ機能のテスト

### API統合テスト（保留中）
- `test_llm.py`：LLM APIエンドポイントの統合テスト
- APIテストは依存関係を適切にモック化するための追加設定が必要

## 今後の機能拡張

1. **プロバイダー実装**
   - OpenAI、Anthropic、Geminiサービスの実装
   - プロバイダー固有のエラーに対する適切なエラーハンドリングの追加

2. **ストリーミングサポート**
   - 互換性のあるプロバイダーのストリーミングレスポンスサポートの追加
   - SSE（Server-Sent Events）エンドポイントの実装

3. **コンテキスト最適化**
   - 大きなドキュメントのセマンティックチャンキングの改善
   - コンテキスト優先順位付けのための関連性スコアリングの実装

4. **モニタリングと可観測性**
   - トークン使用量の詳細なロギングの追加
   - パフォーマンスメトリクスの実装

5. **セキュリティ強化**
   - コンテンツフィルタリングとモデレーション
   - レート制限とクォータ

## 結論

LLMサービス実装は、ドキュメントAIヘルパーバックエンドにLLM機能を統合するための堅固な基盤を提供します。モジュラー設計により、新しいプロバイダーや機能の簡単な拡張が可能です。

すべてのコアコンポーネントはユニットテストされ、期待通りに機能しています。統合テストと実際のプロバイダー実装が実装を完成させるための次のステップです。
