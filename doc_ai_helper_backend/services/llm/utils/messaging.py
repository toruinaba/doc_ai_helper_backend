"""
Messaging utilities for LLM services.

This module provides functionality for conversation history management and system prompt building.
Combines system prompt building with conversation utilities for better organization.
"""

import hashlib
import json
import logging
import time
from typing import Dict, Any, Optional, List, Tuple, Union
from pathlib import Path
from datetime import datetime

from doc_ai_helper_backend.models.llm import MessageItem, MessageRole
from doc_ai_helper_backend.models.repository_context import (
    RepositoryContext,
    DocumentMetadata,
    RepositoryContextSummary,
)
from .tokens import (
    estimate_message_tokens,
    estimate_conversation_tokens,
    DEFAULT_MAX_TOKENS,
)

logger = logging.getLogger(__name__)

# === Conversation History Management ===


def optimize_conversation_history(
    history: List[MessageItem],
    max_tokens: int = DEFAULT_MAX_TOKENS,
    preserve_recent: int = 2,
    encoding_name: str = "cl100k_base",
) -> Tuple[List[MessageItem], Dict[str, Any]]:
    """
    ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å†…ã«åã¾ã‚‹ã‚ˆã†ä¼šè©±å±¥æ­´ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚

    ä¼šè©±å±¥æ­´ãŒæŒ‡å®šã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¶…ãˆã‚‹å ´åˆã€
    æœ€ã‚‚å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰å‰Šé™¤ã—ã¦ã„ãã€‚
    ãŸã ã—ã€æŒ‡å®šã•ã‚ŒãŸæ•°ã®æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å¸¸ã«ä¿æŒã™ã‚‹ã€‚

    Args:
        history: æœ€é©åŒ–ã™ã‚‹ä¼šè©±å±¥æ­´
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        preserve_recent: å¸¸ã«ä¿æŒã™ã‚‹æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
        encoding_name: ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å

    Returns:
        Tuple[List[MessageItem], Dict[str, Any]]: æœ€é©åŒ–ã•ã‚ŒãŸä¼šè©±å±¥æ­´ã¨æœ€é©åŒ–æƒ…å ±
    """
    if not history:
        return [], {"was_optimized": False, "reason": "Empty history"}

    # å…ƒã®å±¥æ­´ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
    original_tokens = estimate_conversation_tokens(history, encoding_name)

    # æœ€é©åŒ–ãŒä¸è¦ãªå ´åˆ
    if original_tokens <= max_tokens:
        optimization_info = {
            "was_optimized": False,
            "reason": "Within token limit",
            "original_message_count": len(history),
            "optimized_message_count": len(history),
            "original_tokens": original_tokens,
            "optimized_tokens": original_tokens,
        }
        return history.copy(), optimization_info

    # æœ€æ–°ã®Nå€‹ã‚’ä¿å­˜ã™ã‚‹ãŸã‚ã€å±¥æ­´ã‚’é€†é †ã«ã™ã‚‹
    preserved_messages = (
        history[-preserve_recent:] if len(history) > preserve_recent else history.copy()
    )
    remaining_messages = (
        history[:-preserve_recent] if len(history) > preserve_recent else []
    )

    # ä¿å­˜ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
    preserved_tokens = estimate_conversation_tokens(preserved_messages, encoding_name)

    # æ®‹ã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    remaining_token_budget = max_tokens - preserved_tokens

    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«ä½™è£•ãŒã‚ã‚‹å ´åˆã¯ã€å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚‚å¯èƒ½ãªé™ã‚Šå«ã‚ã‚‹
    optimized_history = []

    # æ–°ã—ã„é †ã«è¿½åŠ ã—ã¦ã„ã
    for message in reversed(remaining_messages):
        message_tokens = estimate_message_tokens(message, encoding_name)
        if remaining_token_budget >= message_tokens:
            optimized_history.append(message)
            remaining_token_budget -= message_tokens
        else:
            break

    # æ™‚ç³»åˆ—é †ã«æˆ»ã™
    optimized_history.reverse()

    # ä¿å­˜ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    optimized_history.extend(preserved_messages)

    # æœ€é©åŒ–æƒ…å ±ã‚’ä½œæˆ
    optimized_tokens = estimate_conversation_tokens(optimized_history, encoding_name)
    optimization_info = {
        "was_optimized": True,
        "optimization_method": "truncation",
        "original_message_count": len(history),
        "optimized_message_count": len(optimized_history),
        "original_tokens": original_tokens,
        "optimized_tokens": optimized_tokens,
        "messages_removed": len(history) - len(optimized_history),
    }

    logger.debug(
        f"Optimized conversation history: {len(history)} messages -> {len(optimized_history)} messages, "
        f"tokens: ~{optimized_tokens}/{max_tokens}"
    )

    return optimized_history, optimization_info


def format_conversation_for_provider(
    conversation_history: List[MessageItem], provider: str
) -> List[Dict[str, Any]]:
    """
    ä¼šè©±å±¥æ­´ã‚’ç‰¹å®šã®ãƒ—ãƒ­ãƒã‚¤ãƒ€å½¢å¼ã«å¤‰æ›ã™ã‚‹ã€‚

    Args:
        conversation_history: å¤‰æ›ã™ã‚‹ä¼šè©±å±¥æ­´
        provider: LLMãƒ—ãƒ­ãƒã‚¤ãƒ€å ('openai', 'anthropic', 'ollama' ãªã©)

    Returns:
        List[Dict[str, Any]]: ãƒ—ãƒ­ãƒã‚¤ãƒ€å½¢å¼ã®ä¼šè©±å±¥æ­´
    """
    if not conversation_history:
        return []

    if provider.lower() == "openai":
        # OpenAIå½¢å¼: {"role": "user|assistant|system", "content": "..."}
        return [
            {"role": msg.role.value, "content": msg.content}
            for msg in conversation_history
        ]

    elif provider.lower() == "anthropic":
        # Anthropicå½¢å¼: Claudeç”¨ã®å½¢å¼
        formatted = []
        for msg in conversation_history:
            if msg.role.value == "user":
                formatted.append({"role": "human", "content": msg.content})
            elif msg.role.value == "assistant":
                formatted.append({"role": "assistant", "content": msg.content})
            elif msg.role.value == "system":
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯æœ€åˆã«ä¸€ã¤ã ã‘
                if not formatted or formatted[0].get("role") != "system":
                    formatted.insert(0, {"role": "system", "content": msg.content})
        return formatted

    elif provider.lower() == "ollama":
        # Ollamaå½¢å¼: {"role": "user|assistant|system", "content": "..."}
        # åŸºæœ¬çš„ã«OpenAIã¨åŒã˜
        return [
            {"role": msg.role.value, "content": msg.content}
            for msg in conversation_history
        ]

    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯OpenAIå½¢å¼ã«æº–ãšã‚‹
        logger.warning(f"Unknown provider '{provider}', using OpenAI format as default")
        return [
            {"role": msg.role.value, "content": msg.content}
            for msg in conversation_history
        ]


async def summarize_conversation_history(
    history: List[MessageItem],
    llm_service: Any,  # LLMServiceBaseã®å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’é¿ã‘ã‚‹ãŸã‚Anyã‚’ä½¿ç”¨
    max_messages_to_keep: int = 6,
    summary_prompt_template: Optional[str] = None,
) -> Tuple[List[MessageItem], Dict[str, Any]]:
    """
    é•·ã„ä¼šè©±å±¥æ­´ã‚’è¦ç´„ã—ã¦ã€æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨è¦ç´„ã‚’çµ„ã¿åˆã‚ã›ãŸæœ€é©åŒ–æ¸ˆã¿å±¥æ­´ã‚’è¿”ã™ã€‚

    Args:
        history: è¦ç´„ã™ã‚‹ä¼šè©±å±¥æ­´
        llm_service: è¦ç´„ã«ä½¿ç”¨ã™ã‚‹LLMã‚µãƒ¼ãƒ“ã‚¹
        max_messages_to_keep: è¦ç´„å¯¾è±¡å¤–ã¨ã—ã¦ä¿æŒã™ã‚‹æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
        summary_prompt_template: è¦ç´„ã«ä½¿ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

    Returns:
        Tuple[List[MessageItem], Dict[str, Any]]:
            æœ€é©åŒ–ã•ã‚ŒãŸä¼šè©±å±¥æ­´ã¨æœ€é©åŒ–æƒ…å ±
    """
    if len(history) <= max_messages_to_keep:
        return history, {"optimization_applied": False, "reason": "Below threshold"}

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†é›¢
    system_messages = [msg for msg in history if msg.role == MessageRole.SYSTEM]
    conversation_messages = [msg for msg in history if msg.role != MessageRole.SYSTEM]

    if len(conversation_messages) <= max_messages_to_keep:
        return history, {
            "optimization_applied": False,
            "reason": "Below threshold after system separation",
        }

    # æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿æŒ
    recent_messages = conversation_messages[-max_messages_to_keep:]
    messages_to_summarize = conversation_messages[:-max_messages_to_keep]

    if not messages_to_summarize:
        return history, {
            "optimization_applied": False,
            "reason": "No messages to summarize",
        }

    # è¦ç´„ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æº–å‚™
    if summary_prompt_template is None:
        summary_prompt_template = """ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªæ–‡è„ˆã¨æƒ…å ±ã‚’ä¿æŒã—ãªãŒã‚‰ã€ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ä¼šè©±å±¥æ­´:
{conversation_text}

è¦ç´„:"""

    # ä¼šè©±å±¥æ­´ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    conversation_text = "\n".join(
        [f"{msg.role.value}: {msg.content}" for msg in messages_to_summarize]
    )

    summary_prompt = summary_prompt_template.format(conversation_text=conversation_text)

    try:
        # LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦è¦ç´„ã‚’ç”Ÿæˆ
        summary_response = await llm_service.query(
            summary_prompt,
            conversation_history=None,  # è¦ç´„æ™‚ã¯å±¥æ­´ã‚’ä½¿ç”¨ã—ãªã„
            options={"temperature": 0.3, "max_tokens": 500},  # è¦ç´„ã¯ä¸€è²«æ€§ã‚’é‡è¦–
        )

        # è¦ç´„ã‚’å«ã‚€æœ€é©åŒ–æ¸ˆã¿å±¥æ­´ã‚’ä½œæˆ
        summary_message = MessageItem(
            role=MessageRole.ASSISTANT,
            content=f"[ä¼šè©±è¦ç´„] {summary_response.content}",
            timestamp=datetime.now(),
        )

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ + è¦ç´„ + æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        optimized_history = system_messages + [summary_message] + recent_messages

        optimization_info = {
            "optimization_applied": True,
            "original_message_count": len(history),
            "optimized_message_count": len(optimized_history),
            "summarized_message_count": len(messages_to_summarize),
            "kept_recent_message_count": len(recent_messages),
            "summary_tokens": estimate_message_tokens(summary_message),
        }

        return optimized_history, optimization_info

    except Exception as e:
        logger.error(f"Failed to summarize conversation history: {str(e)}")
        # è¦ç´„ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤ã—ã¦æœ€æ–°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ä¿æŒ
        fallback_history = system_messages + recent_messages
        optimization_info = {
            "optimization_applied": True,
            "optimization_method": "fallback_truncation",
            "original_message_count": len(history),
            "optimized_message_count": len(fallback_history),
            "error": str(e),
        }

        return fallback_history, optimization_info


# === System Prompt Classes (kept as is from system_prompt_builder.py) ===

"""
System prompt builder for document-aware LLM interactions.

This module provides functionality to build context-aware system prompts
that include repository and document information for enhanced LLM responses.
"""

import hashlib
import json
import logging
import time
from typing import Dict, Any, Optional, List
from pathlib import Path

from doc_ai_helper_backend.models.repository_context import (
    RepositoryContext,
    DocumentMetadata,
    RepositoryContextSummary,
)

logger = logging.getLogger(__name__)


class SystemPromptCache:
    """Simple in-memory cache for system prompts."""

    def __init__(self, ttl_seconds: int = 3600):
        """
        Initialize cache.

        Args:
            ttl_seconds: Time to live for cached prompts in seconds
        """
        self.cache: Dict[str, str] = {}
        self.cache_times: Dict[str, float] = {}
        self.ttl_seconds = ttl_seconds

    def get(self, key: str) -> Optional[str]:
        """Get cached prompt if valid."""
        if key not in self.cache:
            return None

        # Check TTL
        if time.time() - self.cache_times[key] > self.ttl_seconds:
            self._remove(key)
            return None

        return self.cache[key]

    def set(self, key: str, value: str):
        """Cache a prompt."""
        self.cache[key] = value
        self.cache_times[key] = time.time()

    def _remove(self, key: str):
        """Remove a cached item."""
        self.cache.pop(key, None)
        self.cache_times.pop(key, None)

    def clear(self):
        """Clear all cached items."""
        self.cache.clear()
        self.cache_times.clear()

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        current_time = time.time()
        valid_items = sum(
            1
            for cache_time in self.cache_times.values()
            if current_time - cache_time <= self.ttl_seconds
        )

        return {
            "total_items": len(self.cache),
            "valid_items": valid_items,
            "expired_items": len(self.cache) - valid_items,
            "ttl_seconds": self.ttl_seconds,
        }


class SystemPromptBuilder:
    """
    Base system prompt builder.

    Builds context-aware system prompts using repository and document information.
    """

    def __init__(self, cache_ttl: int = 3600):
        """
        Initialize the builder.

        Args:
            cache_ttl: Cache time-to-live in seconds
        """
        self.cache = SystemPromptCache(cache_ttl)
        self.templates: Dict[str, Dict[str, Any]] = {}
        self._load_templates()

    def _load_templates(self):
        """Load system prompt templates."""
        # Default templates will be loaded from JSON file
        # For now, provide default templates
        self.templates = self._get_default_templates()

    def _get_default_templates(self) -> Dict[str, Dict[str, Any]]:
        """Get default system prompt templates."""
        return {
            "minimal_context_ja": {
                "template": """ã‚ãªãŸã¯ {repository_context.owner}/{repository_context.repo} ãƒªãƒã‚¸ãƒˆãƒªã‚’æ‰±ã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ç¾åœ¨ä½œæ¥­ä¸­ã®ãƒªãƒã‚¸ãƒˆãƒª: {repository_context.owner}/{repository_context.repo}

GitHubãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€ç‰¹ã«æŒ‡å®šãŒãªã„é™ã‚Šè‡ªå‹•çš„ã« "{repository_context.owner}/{repository_context.repo}" ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚æ—¥æœ¬èªã§è‡ªç„¶ã«å¿œç­”ã—ã€æŠ€è¡“çš„ãªå†…å®¹ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚""",
                "required_context": ["repository_context"],
            },
            "contextual_document_assistant_ja": {
                "template": """ã‚ãªãŸã¯ãƒªãƒã‚¸ãƒˆãƒª {repository_context.owner}/{repository_context.repo} ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ‰±ã†å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼š
- ãƒªãƒã‚¸ãƒˆãƒª: {repository_context.service}:{repository_context.owner}/{repository_context.repo} (ãƒ–ãƒ©ãƒ³ãƒ: {repository_context.ref})
- ç¾åœ¨è¡¨ç¤ºä¸­ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {repository_context.current_path}
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå½¢å¼: {document_metadata.type}
- æœ€çµ‚æ›´æ–°: {document_metadata.last_modified}

{document_content_section}

é‡è¦ãªæŒ‡ç¤ºï¼š
1. GitHubã®Issueã‚„Pull Requestã®ä½œæˆã‚’æ±‚ã‚ã‚‰ã‚ŒãŸå ´åˆã¯ã€è‡ªå‹•çš„ã« "{repository_context.owner}/{repository_context.repo}" ãƒªãƒã‚¸ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
2. ã€Œã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€ã€Œã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã€ã¨è¨€åŠã•ã‚ŒãŸå ´åˆã¯ã€{repository_context.current_path} ã‚’æŒ‡ã—ã¦ã„ã¾ã™
3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å…·ä½“çš„ãªå†…å®¹ã‚’å‚ç…§ã—ã¦ã€è©³ç´°ã§å®Ÿç”¨çš„ãªææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„
4. æ”¹å–„ææ¡ˆã‚„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã€è¡¨ç¤ºä¸­ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ–‡è„ˆã«åŸºã¥ã„ã¦è¡Œã£ã¦ãã ã•ã„
5. æ—¥æœ¬èªã§è‡ªç„¶ã«å¿œç­”ã—ã€æŠ€è¡“ç”¨èªã¯é©åˆ‡ã«æ—¥æœ¬èªåŒ–ã™ã‚‹ã‹ä½µè¨˜ã—ã¦ãã ã•ã„

åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ï¼š
- create_github_issue: ç¾åœ¨ã®ãƒªãƒã‚¸ãƒˆãƒªã«Issueã‚’ä½œæˆ
- create_github_pull_request: ç¾åœ¨ã®ãƒªãƒã‚¸ãƒˆãƒªã«Pull Requestã‚’ä½œæˆ
- validate_github_repository: ãƒªãƒã‚¸ãƒˆãƒªã‚¢ã‚¯ã‚»ã‚¹ã®ç¢ºèª

å¯¾è©±ä¾‹ï¼š
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª¬æ˜ãŒåˆ†ã‹ã‚Šã«ãã„éƒ¨åˆ†ãŒã‚ã‚Šã¾ã™ã€
  â†’ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã‚’åˆ†æã—ã€å…·ä½“çš„ãªæ”¹å–„ç‚¹ã‚’ç‰¹å®šã—ã¦Issueã¨ã—ã¦ææ¡ˆ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œè‹±èªã®éƒ¨åˆ†ã‚’æ—¥æœ¬èªåŒ–ã—ã¦ã»ã—ã„ã€
  â†’ è©²å½“ç®‡æ‰€ã‚’ç‰¹å®šã—ã€æ—¥æœ¬èªåŒ–ã®Pull Requestã‚’ææ¡ˆ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œã“ã®APIã®ä½¿ç”¨ä¾‹ã‚’è¿½åŠ ã—ãŸã„ã€
  â†’ é©åˆ‡ãªå ´æ‰€ã«ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã™ã‚‹Pull Requestã‚’ææ¡ˆ""",
                "required_context": ["repository_context", "document_metadata"],
            },
        }

    def generate_cache_key(
        self,
        repository_context: Optional[RepositoryContext] = None,
        document_metadata: Optional[DocumentMetadata] = None,
        document_content: Optional[str] = None,
        template_id: str = "contextual_document_assistant_ja",
    ) -> str:
        """
        Generate cache key for system prompt.

        Args:
            repository_context: Repository context
            document_metadata: Document metadata
            document_content: Document content (hashed for cache key)
            template_id: Template identifier

        Returns:
            Cache key string
        """
        # Create a deterministic cache key
        key_parts = [template_id]

        if repository_context:
            key_parts.extend(
                [
                    repository_context.service.value,
                    repository_context.owner,
                    repository_context.repo,
                    repository_context.ref,
                    repository_context.current_path or "root",
                ]
            )

        if document_metadata:
            key_parts.extend(
                [
                    document_metadata.type.value,
                    document_metadata.last_modified or "unknown",
                    str(document_metadata.file_size or 0),
                ]
            )

        if document_content:
            # Hash the content to avoid extremely long keys
            content_hash = hashlib.md5(document_content.encode("utf-8")).hexdigest()[:8]
            key_parts.append(content_hash)

        # Create deterministic hash
        key_string = "|".join(key_parts)
        cache_key = hashlib.sha256(key_string.encode("utf-8")).hexdigest()[:16]

        return cache_key

    def sanitize_document_content(self, content: str, max_length: int = 8000) -> str:
        """
        Sanitize document content for system prompt inclusion.

        Args:
            content: Raw document content
            max_length: Maximum length in characters

        Returns:
            Sanitized content
        """
        if not content:
            return ""

        # Truncate if too long
        if len(content) > max_length:
            # Try to truncate at a reasonable boundary
            truncated = content[:max_length]

            # Find last complete line
            last_newline = truncated.rfind("\n")
            if last_newline > max_length // 2:  # Don't truncate too aggressively
                truncated = truncated[:last_newline]

            content = truncated + "\n\n... (å†…å®¹ãŒé•·ã„ãŸã‚çœç•¥ã•ã‚Œã¾ã—ãŸ)"

        # Escape markdown code blocks to prevent prompt injection
        content = content.replace("```", "\\`\\`\\`")

        return content

    def build_system_prompt(
        self,
        repository_context: Optional[RepositoryContext] = None,
        document_metadata: Optional[DocumentMetadata] = None,
        document_content: Optional[str] = None,
        template_id: str = "contextual_document_assistant_ja",
    ) -> str:
        """
        Build system prompt with context.

        Args:
            repository_context: Repository context
            document_metadata: Document metadata
            document_content: Document content
            template_id: Template identifier

        Returns:
            Generated system prompt
        """
        # Check cache first
        cache_key = self.generate_cache_key(
            repository_context, document_metadata, document_content, template_id
        )

        cached_prompt = self.cache.get(cache_key)
        if cached_prompt:
            logger.debug(f"Using cached system prompt (key: {cache_key})")
            return cached_prompt

        # Get template
        template_data = self.templates.get(template_id)
        if not template_data:
            logger.warning(f"Template not found: {template_id}, using default")
            template_id = "minimal_context_ja"
            template_data = self.templates[template_id]

        template = template_data["template"]
        required_context = template_data.get("required_context", [])

        # Check required context
        context_vars = {}

        if repository_context:
            context_vars["repository_context"] = repository_context

        if document_metadata:
            context_vars["document_metadata"] = document_metadata

        # Validate required context
        missing_context = [ctx for ctx in required_context if ctx not in context_vars]

        if missing_context:
            logger.warning(
                f"Missing required context: {missing_context}, using minimal template"
            )
            template_id = "minimal_context_ja"
            template = self.templates[template_id]["template"]

        # Build document content section
        document_content_section = ""
        if document_content and template_id != "minimal_context_ja":
            sanitized_content = self.sanitize_document_content(document_content)
            if sanitized_content:
                document_content_section = f"""è¡¨ç¤ºä¸­ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ï¼š
```{document_metadata.type.value if document_metadata else 'text'}
{sanitized_content}
```"""
            else:
                document_content_section = (
                    "â€»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã¯å¤§ãã™ãã‚‹ãŸã‚è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
                )

        # Format template
        try:
            formatted_prompt = template.format(
                repository_context=repository_context,
                document_metadata=document_metadata,
                document_content_section=document_content_section,
            )
        except Exception as e:
            logger.error(f"Error formatting template {template_id}: {e}")
            # Fallback to minimal template
            fallback_template = self.templates["minimal_context_ja"]["template"]
            formatted_prompt = fallback_template.format(
                repository_context=repository_context
            )

        # Cache the result
        self.cache.set(cache_key, formatted_prompt)

        # Log generation
        if repository_context:
            summary = RepositoryContextSummary.from_context(
                repository_context, document_metadata
            )
            logger.info(
                f"Generated system prompt: {summary.repository} - {template_id}"
            )

        return formatted_prompt

    def clear_cache(self):
        """Clear the system prompt cache."""
        self.cache.clear()
        logger.info("System prompt cache cleared")

    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        return self.cache.get_stats()


class JapaneseSystemPromptBuilder(SystemPromptBuilder):
    """
    Japanese-optimized system prompt builder.

    Specialized for Japanese documentation and development workflows.
    """

    def _get_default_templates(self) -> Dict[str, Dict[str, Any]]:
        """Get Japanese-optimized templates."""
        templates = super()._get_default_templates()

        # Add specialized Japanese templates
        templates.update(
            {
                "documentation_specialist_ja": {
                    "template": """ã‚ãªãŸã¯æ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å°‚é–€å®¶ã¨ã—ã¦ã€{repository_context.owner}/{repository_context.repo} ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ”¹å–„ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

ç¾åœ¨ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {repository_context.current_path}

{document_content_section}

å°‚é–€é ˜åŸŸï¼š
âœ… æ—¥æœ¬èªã®æ–‡ç« æ§‹æˆã¨å¯èª­æ€§ã®æ”¹å–„
âœ… æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ§‹é€ åŒ–ã¨æ•´ç†
âœ… ã‚³ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«ã®èª¬æ˜ã¨æ”¹å–„
âœ… ç”¨èªã®çµ±ä¸€ã¨é©åˆ‡ãªæ—¥æœ¬èªåŒ–
âœ… ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸæƒ…å ±è¨­è¨ˆ

æ”¹å–„ææ¡ˆã®æŒ‡é‡ï¼š
1. èª­ã¿æ‰‹ã®ç«‹å ´ã«ç«‹ã£ãŸåˆ†ã‹ã‚Šã‚„ã™ã•ã‚’é‡è¦–
2. å…·ä½“çš„ã§å®Ÿè¡Œå¯èƒ½ãªæ”¹å–„æ¡ˆã‚’æç¤º
3. æŠ€è¡“ç”¨èªã¯æ—¥æœ¬èªã§ã®èª¬æ˜ã‚’ä½µè¨˜
4. æ§‹é€ çš„ãªå•é¡ŒãŒã‚ã‚Œã°å…¨ä½“çš„ãªå†æ§‹æˆã‚’ææ¡ˆ
5. å®Ÿéš›ã®GitHub Issueã‚„Pull Requestã¨ã—ã¦å…·ä½“åŒ–

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«é–¢ã™ã‚‹è³ªå•ã‚„æ”¹å–„è¦æœ›ãŒã‚ã‚Œã°ã€å°‚é–€çš„ãªè¦³ç‚¹ã‹ã‚‰è©³ç´°ã«åˆ†æã—ã€å®Ÿè¡Œå¯èƒ½ãªå½¢ã§ææ¡ˆã—ã¾ã™ã€‚""",
                    "required_context": ["repository_context", "document_metadata"],
                },
                "code_reviewer_ja": {
                    "template": """ã‚ãªãŸã¯ {repository_context.owner}/{repository_context.repo} ã®ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚

ç¾åœ¨ç¢ºèªä¸­ã®ãƒ•ã‚¡ã‚¤ãƒ«: {repository_context.current_path}

{document_content_section}

ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦³ç‚¹ï¼š
ğŸ” ã‚³ãƒ¼ãƒ‰ã®å“è³ªã¨å¯èª­æ€§
ğŸ” æ—¥æœ¬èªã‚³ãƒ¡ãƒ³ãƒˆã®é©åˆ‡æ€§
ğŸ” å‘½åè¦å‰‡ã®ä¸€è²«æ€§
ğŸ” ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ã®å……å®Ÿåº¦
ğŸ” ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®é©ç”¨

ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å½¢å¼ï¼š
- æ”¹å–„ç‚¹ã¯å…·ä½“çš„ãªä¿®æ­£æ¡ˆã¨ã¨ã‚‚ã«æç¤º
- è‰¯ã„ç‚¹ã‚‚ç©æ¥µçš„ã«è©•ä¾¡
- å­¦ç¿’ã®ãŸã‚ã®è¿½åŠ èª¬æ˜ã‚’æä¾›
- å¿…è¦ã«å¿œã˜ã¦Issueã‚„Pull Requestã¨ã—ã¦å…·ä½“åŒ–

ã‚³ãƒ¼ãƒ‰ã«é–¢ã™ã‚‹è³ªå•ã‚„æ”¹å–„è¦æœ›ãŒã‚ã‚Œã°ã€æŠ€è¡“çš„ã«æ­£ç¢ºã§å®Ÿç”¨çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚""",
                    "required_context": ["repository_context", "document_metadata"],
                },
            }
        )

        return templates

    def select_appropriate_template(
        self,
        repository_context: RepositoryContext,
        document_metadata: Optional[DocumentMetadata] = None,
    ) -> str:
        """
        Select appropriate template based on context.

        Args:
            repository_context: Repository context
            document_metadata: Document metadata

        Returns:
            Template ID
        """
        if not document_metadata:
            return "minimal_context_ja"

        # Check if it's a documentation file
        if document_metadata.is_documentation:
            if "README" in (repository_context.current_path or "").upper():
                return "documentation_specialist_ja"
            elif any(
                keyword in (repository_context.current_path or "").lower()
                for keyword in ["doc", "docs", "api", "spec", "guide", "manual"]
            ):
                return "documentation_specialist_ja"

        # Check if it's a code file
        elif document_metadata.is_code_file:
            return "code_reviewer_ja"

        # Default to contextual assistant
        return "contextual_document_assistant_ja"
